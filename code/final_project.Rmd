---
title: "Crime and Communities"
author:
  - Justin Han
output:
  pdf_document: default
  html_notebook: default
---

```{r, include=FALSE}
library(readr)
library(dplyr)
library(DataExplorer)
library(pls)
library(glmnet)
```

```{r}
CC <- read_csv("../data/crime_and_communities_data.csv")
CC <- as.data.frame(CC)
```

## Dataset exploration

**After the dataset has been imported, we want to preview it to get an idea of what we will be working with.**
```{r}
head(CC, 10)
```

**Next, we will learn more about our dataset's nuances. Some of the attributes we would like to pay attention to include the dimension, the number of categorical versus numerical variables, and how many missing values there are. Below is a high level overview which includes the attributes of interest as well as some other useful to know information about our data, such as memory. This can be done by using the `glimpse` function from the `dplyr` package and the `introduce` function from the `DataExploration` package.**
```{r}
#This is like a transposed version of print: columns run down the page, and data runs across. This makes it possible to see every column in a data frame, essentially trying to show as much data as possible
glimpse(CC)

#summarize key attributes of the data
introduce(CC)
```
Here, "complete_rows" refers to the number of rows without any missing values, "all_missing_columns" refers to the number of missing columns (the entire column is NA), "total_observations" refers to each value in the dataset (including missing values), and "discrete_columns" refers to the number of categorical variables in our data. From our findings, we see that there are no cateogorial variables. Additionally, out of the 249250 values, there are a total of 36851 missing values. 

**We can visualize our findings with a barplot that gives us the proportions of each attribute. The `plot_intro` function from the `DataExploration` package lets us do just that.**
```{r}
plot_intro(CC)
```

From our visualization, it becomes clear that:  
*1. Only roughly 16% of all rows are not completely missing*   
*2. About 15% of the values in the dataset are missing*    
Missing values definitely will cause problems. Hence the next step will be to take a closer look at what is missing and what we can do to alleviate the problem. 
  
**The following code breaks down the number of missing values by variable with a for loop, looping over each coloumn in our dataset.**  
```{r}
na_per_col <- c()
for(i in 1:ncol(CC)){
  na_per_col[i] <- sum(is.na(CC[,i]))
}
names(na_per_col) <- names(CC)
na_per_col
```

**This can be visualized as well with a barplot which illustrates the proportion of missing values for each variable given that the variable contains missing values. The `plot_missing` function from the `DataExploration` package allows us to accomplish this.**
```{r}
plot_missing(CC[,na_per_col!= 0])
```

From our illustration, we observe that out of the 23 variables that contain missing values, all of them except for "OtherPerCap" has a significant portion of missing values. Because those variables are mostly missing values, we can drop them using the `drop_columns` function since they probably will not provide us with much information. Doing this drop reduces the number of variables in our dataset to 103 (previously 125) as shown with the `dim` function. 
```{r}
cleaned_CC <- drop_columns(CC, names(na_per_col)[na_per_col == 1675])
cleaned_CC <- as.data.frame(cleaned_CC)
dim(cleaned_CC)
```

**Now that we have learned and cleaned our dataset a little bit, we can get a general summary of our data using the `summary` function. This will provide us with some useful summary statistics of our variables as well as give us insight as to how they are distributed. We can visualize these distributions by plotting the histograms corresponding to each variable.**  
```{r}
summary(cleaned_CC)
par(mfrow=c(2,4))
for (name in names(cleaned_CC)){
  hist(cleaned_CC[ ,name], col ='gray80', main =paste(name), xlab ='', las = 1)
}
```

**We notice that many of the variables have a skewed distribution as depicted by the shapes of the histograms (this indicates that we might have to scale our data which will be addressed later). Having seen how each variable behaves on its own, we can even go one step further and examine how each variable behaves with respect to one another. Learning which variables are correlated can help us gain intuition behind why certain variables can be dropped. Below is a matrix of correlations (since the matrix is really large, we will use the `head` function to just view a small portion of it).**  
```{r}
head(cor(cleaned_CC))
```

## Regression Task
#### _Our goal is to develop a model to predict "ViolentCrimesPerPop"_

**Since there are so many variables in our dataset, we hope to find an appropriate model using dimension reduction methods or shrinkage methods so that we can predict our desired response in a "friendly" way. In particular, we will use methods such as PC regression, PLS regression, Ridge regression, and Lasso regression. Addtionally, we will use cross-validation to select tuning parameters and utilize the three-way holdout method to perform model selection and model assessment.**
  
**We begin by defining our response vector and feature matrix. Also, recall that there is one column ("OtherPerCap") which contains a single missing value of which we chose to keep. To deal with this missing value, we will impute the missing value with its respective column mean.**  
```{r}
#response vector
y <- cleaned_CC$ViolentCrimesPerPop
response <- "y"

#feature matrix with replaced missing value
X <- subset(cleaned_CC, select = -c(ViolentCrimesPerPop))
X$OtherPerCap[is.na(X$OtherPerCap)] <- mean(X$OtherPerCap, na.rm = TRUE)
predictors <- names(X)

#combine feature matrix and response vector for convenience
dat <- as.data.frame(cbind(X,y))
```

#### Three-way hold-out method

**Before we begin implementing any of the regularization methods, we first split the data into three different parts:**  
*1. Training set: 60% of the data (chosen at random)*  
*2. Validation set: 20% of the data (i.e. one half of the remaining 40% not in training, chosen at random)*   
*3. Test set: 20% of the data (i.e. the other half of the remaining 40% not in training, chosen at random)*  
```{r}
set.seed(1)
train_prop <- 0.6
total <- nrow(dat)

#training set
train <- sample(1:total, floor(total*train_prop))
testval <- c(1:total)[-train]

#validation set
validation <- sample(testval, size = floor(length(testval)/2))

#test set
test <- c(1:total)[-c(train, validation)]
```

#### PCA 

**After successfully prepping our data, we standardize it in order to ensure that the significance of each variable is properly captured relative to one another. The first thing that we would like to do is attempt to reduce the dimensionality by performing PCA.**  
```{r}
#standardize feature matrix
X <- scale(X)

#compute variance-covariance matrix
n <- nrow(X)
S <- (1/(n-1)) * t(as.matrix(X)) %*% as.matrix(X)

#perform eigenvalue decomposition
EVD <- eigen(S)

#eigenvalues of S (the variances of each PC)
evalues <- EVD$values
head(evalues)

#eigenvectors of S (loadings)
evectors <- EVD$vectors
head(evectors)

#principal components (each PC is a linear combination of each of the original features)
pcs <- X %*% evectors
colnames(pcs) <-paste0('PC', 1:ncol(pcs))
head(pcs[,1:4])

#high level summary of doing PCA
pc <- princomp(X, scores = TRUE)
summary(pc)
```

**We can summarize our findings visually with a plot which will give us a sense of what PCs are important and the amount of variance each captures.**  
```{r}
plot(pc, type = "l")
```

The plot indicates that we do not need to use all of the PCs to get good results since the first couple of PCs already capture most of the variance of our original data. In fact, if we look at the cumulative proportions, we notice that the first 17 PCs already capture roughly 85% of the total variance and the first 34 captures about 95%! This is a big step forward since it shows a significant drop in the number of PCs we would need (total of 102 PCs), reducing dimensionality while still capturing as much variability of the original data as possible. We can build our model using just a few PCs and still get good results; this ultimately sets the stage for dimension reduction techniques like PCR as well as PLSR and gives us a reason as to why we should utilize them.

#### Principal Component Regression

**In the first method, we will use Principal Component Regression (PCR). The idea is that we can fit a least squares model on a small number of PCs and obtain coefficients that give us a good model to use in predicting our response. We use the training set to fit PCR. The `pcr` function returns the regression coefficients in terms of the X variables, which simplifies interpretation. We can then visualize how the coefficients grow in terms of the number of retained PCs.**
```{r}
formula <- y ~. 
Xtrain <- scale(dat[train, c(predictors, response)])
Xtrain <- as.data.frame(Xtrain)

#PCR on training data
pcr_fit <- pcr(formula, data = Xtrain, scale = FALSE, ncomp = ncol(X))

PCR <- apply(pcr_fit$coefficients, MARGIN = 3, function(u) u)
matplot(t(PCR), type = "l", xlim = c(1, 102), ylim = c(-1, 1), ylab = "Value", xlab = "PCs")
```

The graph consists of a different line for each coefficient (total of 102 because there are 102 features). Each line captures how each of the coefficients change as the number of retained PCs increases. The coefficients start off stable but as the number of PCs increase, we can see that the values of the coefficients start to grow both in the postive and negative directions. We observe something interesting when the number of PCs reach 60. We can see that at that point, the values of the coefficients begin to fluctuate really rapidly, leading to significant differences between them. 

#### Partial Least Squares Regression

**The next method is Partial Least Squares Regression (PLSR). It is very similar to PCR except in PLS, it makes use of the response in order to identify new features (PLS components) that not only approximate the old features well, but also that are related to the response. The function `plsr` returns the regression coefficients in terms of the X variables, which simplifies interpretation yet again. Similar to what we did with PCR, We can visualize how the coefficients grow in terms of the number of retained PLS components.**
```{r}
#PLSR on training data
plsr_fit <-plsr(formula, data = Xtrain, scale = FALSE, ncomp = ncol(X))

PLSR <- apply(plsr_fit$coefficients, MARGIN = 3,function(u) u)
matplot(t(PLSR), type = "l", xlim = c(1, 102), ylim = c(-1.5, 1.25), ylab = "Value", xlab = "PLS Components")
```

The graph consists of a different line for each coefficient (total of 102 because there are 102 features). Each line captures how each of the coefficients change as the number of retained PLS components increases. Unlike the coefficients in PCR, we notice that the cofficients in PLS experience much greater changes and are much more sensitive to the number of components retained.

#### Ridge Regression

**So far, we fitted two dimension reduction methods on our training data and observed how the regression coefficients change when we retain different numbers of components for PCR and PLSR. We can also use shrinkage methods like Ridge regression (RR) and Lasso regression to examine some interesting effects that the tuning parameter, lambda, has on the regression coefficients. This way, we will be able to have multiple sources of comparision, providing us more insight about the different regularization techniques used. Below is a graph of how the regression coefficients change in terms of lambda for Ridge regression.**
```{r}
#ridge regression on training data
x <- as.matrix(Xtrain[ ,predictors])
y <- Xtrain[ ,response]

ridge_fit <- glmnet(x, y, alpha = 0)

matplot(t(ridge_fit$beta), type = "l", las = 1, ylim = c(-0.1, 0.2), ylab = "Value", xlab = "Lambda")
```

The graph consists of a different line for each coefficient (total of 102 because there are 102 features). Each line captures how each of the coefficients change with varying values of lambda. We notice that the change in value of the cofficients for RR follow a similar shape as that of PLSR and PCR. 

#### Lasso Regression

**Lasso regression is similar to Ridge regression in the sense that both are shrinkage methods that involve the parameter, lambda. However, one disadvantage that Ridge regression has is the fact that RR will include all features (in this case 102) in the final model. Although this may not be a problem for prediction accuracy, it may make model interpretation somewhat challenging especially when the number of features we are working with is large. Lasso regression helps overcome this issue by slightly adjusting the way the penalty is set up. Below is a graph of how the regression coefficients change in terms of lambda for Lasso regression. ** 
```{r}
#lasso regression on training data
lasso_fit <- glmnet(x, y, alpha = 1)

matplot(t(lasso_fit$beta), type = "l", las = 1, ylab = "Value", xlab = "Lambda")
```

The graph consists of a different line for each coefficient (total of 102 because there are 102 features). Each line captures how each of the coefficients change with varying values of lambda. Again, we notice here that the change in value of the cofficients for Lasso follow a similar shape as that of PLSR and PCR. The one thing that stands out about the regression coefficients for Lasso is how sensitive it is to lambda early on.  

#### Cross-Validation

**The question to ask now is: what is the best number of components to have? What is the best lambda to use? The answers to these questions can be found by using k-fold cross validation. The size of k is somewhat arbitrary, but typically 5 or 10 is good. We will go with k=5 here. That means we will have to create folds by splitting the training data into 5 subsets of similar size and then train on the first k-1 folds and reserve the kth fold for "testing". We repeat this process for each fold, training on all folds except for the one that has been reserved for "testing".**
```{r}
#create folds
folds <- 5
if ((length(train)%%folds)==0){
  cuts <- seq(1,length(train), by = length(train)/folds)
} else {
    cuts <- seq(1,length(train), by = floor(length(train)/folds))
    cuts[folds+1] <-length(train)
}

#starting and ending positions of folds
fold_starts <- cuts[-length(cuts)]
fold_ends <- c(cuts[2:(length(cuts)-1)]-1, cuts[length(cuts)])

folds_list <-as.list(1:folds)
for(fold in 1:folds) {
  folds_list[[fold]] <- fold_starts[fold]:fold_ends[fold]
}
```

Once we have created the folds, we will train our methods on the training data and compute cross-validation MSEs for each value of a tuning parameter (the number of components and lambda are examples of tuning parameters). We note the tuning parameters that yield the lowest cross-validation MSE for each model (training MSE for each). The following code uses a for loop to do this process for PCR and PLSR. We also make use of the `cv.glmnet` function to do this process for Ridge regression and Lasso regression.

```{r}
mse_cv_pcr <- c()
mse_cv_plsr <- c()

#pcr and plsr
for(tune in 1:ncol(X)) {
  mse_pcr_aux <- c()
  mse_plsr_aux <- c()
  for(fold in 1:folds) {
    #train with k-1 folds
    Xtrain_folds <- scale(Xtrain[folds_list[[fold]], ])
    Xtrain_folds <- as.data.frame(Xtrain_folds)
    x_aux <- as.matrix(Xtrain_folds[, predictors])
    y_aux <- Xtrain_folds[,response]
    pcr_aux <- pcr(formula, data = Xtrain_folds, scale = FALSE, ncomp = tune)
    plsr_aux <-plsr(formula, data = Xtrain_folds, scale = FALSE, ncomp = tune)
    
    # predict holdout fold
    Xfoldout <- scale(Xtrain[unlist(folds_list[-fold]), predictors])
    Xfoldout <- as.data.frame(Xfoldout)
    yfoldout <- Xtrain[unlist(folds_list[-fold]), response]
    
    pcr_hat <- predict(pcr_aux, Xfoldout, ncomp = tune)
    plsr_hat <- predict(plsr_aux, Xfoldout, ncomp = tune)
    
    #measure performance
    mse_pcr_aux[fold] <- mean((pcr_hat[,,1]-yfoldout)^2)
    mse_plsr_aux[fold] <- mean((plsr_hat[,,1]-yfoldout)^2)
  }
  mse_cv_pcr[tune] <- mean(mse_pcr_aux)
  mse_cv_plsr[tune] <- mean(mse_plsr_aux)
}

lasso <- cv.glmnet(as.matrix(Xtrain[,c(1:102)]), Xtrain[,response], alpha = 1, type.measure = "mse", nfolds = 5)
ridge <- cv.glmnet(as.matrix(Xtrain[,c(1:102)]), Xtrain[,response], alpha = 0, type.measure = "mse", nfolds = 5)

#lasso
lasso_keep <- lasso$nzero[lasso$lambda == lasso$lambda.min]

#ridge
ridge_keep <- ridge$nzero[ridge$lambda == ridge$lambda.min]

#show the best tuning parameter for each method
cv_mse <- data.frame(pcr = mse_cv_pcr, plsr = mse_cv_plsr)
selected_pcr_plsr <- apply(cv_mse, 2, which.min)
keep <- c(selected_pcr_plsr, "ridge" = ridge$lambda.min, "lasso" = lasso$lambda.min)
keep
```

This table summarizes, for every method, the best number of components to keep as well as the best lambda to choose (determined by the lowest cross-validation MSE for each). Now that we have found the best tuning parameters for each model, the next question to ask is: which model is the best one? We will use the validation set to determine the answer. In the next step, we will fit our ideal models (those with the best tuning parameters that we just found for each) on our validation set and compare how each model does.  

```{r}
Xval <- scale(dat[validation, c(predictors, response)])
x_val <- as.matrix(Xval[,predictors])
y_val <- Xval[, response]

pcr_hat <- predict(pcr_fit, x_val, ncomp = keep[1])
plsr_hat <- predict(plsr_fit, x_val, ncomp = keep[2])
las_hat <- predict(lasso_fit, newx = x_val, s = keep[3])
rr_hat <- predict(ridge_fit, newx = x_val, s = keep[4])

mse_val <- c(
  "pcr" = mean((pcr_hat[,,1]-y_val)^2),
  "plsr" = mean((plsr_hat[,,1]-y_val)^2),
  "ridge" = mean((rr_hat[,1]-y_val)^2),
  "lasso" = mean((las_hat[,1]-y_val)^2)
)

mse_val
which.min(mse_val)
```

The model that gives us the lowest validation MSE is the best model out of the bunch; in our case, Ridge Regression yields the lowest validation MSE. Once we have identified the best model, we need to assess its performance using the test set. 

## Final Model and its Performance

```{r}
#test the performance of the best model
Xbest <- scale(dat[c(train, validation),c(predictors, response)])
Xtest <- scale(dat[test,c(predictors, response)])
x_test <- as.matrix(Xtest[, predictors])
y_test <- Xtest[,response]

ridge_best <- glmnet(Xbest[,predictors], Xbest[,response], alpha = 0, lambda = keep[4])
y_hat_test <- predict(ridge_fit, newx = x_test, s = keep[4])
mean((y_hat_test-y_test)^2)
```

The resulting quantity above shows the test MSE for our best model (Ridge regression). We note that the test MSE is greater than the validation MSE, but by a very small amount. We can conclude that our model did a fairly good job in predicting "ViolentCrimesPerPop". For our last step, we will use our entire dataset (training set, validation set, and test set) to fit our crowned model and observe its coefficients.  

```{r}
Xall <- scale(dat)
ridge_model <- glmnet(Xall[,predictors], Xall[,response], alpha = 0, lambda = keep[4])
ridge_model$beta
```
