---
title: \vspace{-5truemm} Crime and Communities \vspace{-5truemm}
author: Justin Han
output:
  pdf_document:
    number_sections: true
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  cache = FALSE)

# load in useful packages
library(readr)
library(dplyr)
library(DataExplorer)
library(pls)
library(glmnet)
library(corrplot)
```

```{r}
# load data
CC <- read_csv("../data/crime_and_communities_data.csv")
CC <- as.data.frame(CC)
```

# Dataset exploration

```{r}
preview <- head(CC, 10)
```

To start, we want to examine and familiarize ourselves with the dataset. More importantly, we want to learn more about our dataset's nuances. Some of the attributes we should pay attention to include the dimension, the number of categorical versus numerical variables, and how many missing values there are. Below is a high level overview which includes the attributes of interest as well as some other useful to know information about our data, such as memory. 

```{r}
# this can be done by using the `glimpse` function from the `dplyr` package and the 
# `introduce` function from the `DataExploration` package.

# this is like a transposed version of print: columns run down the page, and data runs across. 
# this makes it possible to see every column in a data frame, essentially trying to show as much data as possible
glimpse(CC)

# summarize key attributes of the data
introduce(CC)
```

Here, "complete_rows" refers to the number of rows without any missing values, "all_missing_columns" refers to the number of missing columns (the entire column is NA), "total_observations" refers to each value in the dataset (including missing values), and "discrete_columns" refers to the number of categorical variables in our data. From our findings, we see that there are no cateogorial variables. Additionally, out of the $249250$ values, there are a total of $36851$ missing values. 

Our findings can be summarized visually with a barplot that gives us the proportions of each attribute.

```{r, fig.align = "center", fig.pos = "H", fig.height = 3}
# the `plot_intro` function from the `DataExploration` package lets us 
# visualize our findings
plot_intro(CC)
```

From our visualization, it becomes clear that only roughly $16$% of all rows are not completely missing and about $15$% of the values in the dataset are missing. Missing values definitely will cause problems when it comes to accurate analysis. Hence the next step will be to take a closer look at what is missing and what we can do to alleviate the problem. This can be visualized as well with a barplot which illustrates the proportion of missing values for each variable given that the variable contains missing values. 

```{r}
# the following code breaks down the number of missing values by variable with a 
# for loop, looping over each coloumn in our dataset.  
na_per_col <- c()
for(i in 1:ncol(CC)){
  na_per_col[i] <- sum(is.na(CC[,i]))
}
names(na_per_col) <- names(CC)
```

```{r, fig.align = "center", fig.pos = "H", fig.height = 5}
# the `plot_missing` function from the `DataExploration` package allows us to 
# visualize the proportion of missing values
plot_missing(CC[,na_per_col!= 0])
```

From our illustration, we observe that out of the $23$ variables that contain missing values, all of them except for "OtherPerCap" has a significant portion of missing values. Because those variables are mostly missing values, we can drop them using the `drop_columns` function since they probably will not provide us with much information. Doing this drop reduces the number of variables in our dataset to $103$ (previously $125$). 

```{r}
cleaned_CC <- drop_columns(CC, names(na_per_col)[na_per_col == 1675])
cleaned_CC <- as.data.frame(cleaned_CC)
#dim(cleaned_CC)
```

Now that we have learned about and cleaned our dataset, we can visualize a random subset of features just to get a sense of their distributions. Doing this will hopefully give us a rough idea of the properties of the $103$ variables we have available to work with. Below are the histograms of a randomly selected set of predictor variables.

```{r, fig.align = "center", fig.pos = "H", fig.height = 5}
#summary(cleaned_CC)
par(mfrow=c(2,4))

# histograms
subset <- sample(names(cleaned_CC), 8)
for (name in subset){
  hist(cleaned_CC[ ,name], col ='gray80', main = paste(name), xlab ='', las = 1)
}
```

We notice that many of the variables have a skewed distribution as depicted by the shapes of the histograms in the subset (this indicates that we might have to scale our data which will be addressed later). Having seen how each variable behaves on its own, we can even go one step further and examine how each variable behaves with respect to one another. Learning which variables are correlated can help us gain intuition behind why certain variables can be dropped. Below is a plot of the correlations (since the correlation plot consisting of all predictors is really large, we will just examine it for the above subset of predictors). 

```{r, fig.align = "center", fig.pos = "H", fig.height = 5}
# correlation matrix for the above subset of predictors
corr <- cor(cleaned_CC[,subset])
corrplot(corr, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

# Analysis

Our goal is to develop a model to predict "ViolentCrimesPerPop". Since there are so many variables in our dataset, we hope to find an appropriate model using dimension reduction methods or shrinkage methods so that we can predict our desired response in a "friendly" way. In particular, we will use methods such as PC regression, PLS regression, Ridge regression, and Lasso regression. Addtionally, we will use cross-validation to select tuning parameters and utilize the three-way holdout method to perform model selection and model assessment. We begin by defining our response vector and feature matrix. Also, recall that there is one column ("OtherPerCap") which contains a single missing value of which we chose to keep. To deal with this missing value, we will impute the missing value with its respective column mean.

```{r}
# response vector
y <- cleaned_CC$ViolentCrimesPerPop
response <- "y"

# feature matrix with replaced missing value
X <- subset(cleaned_CC, select = -c(ViolentCrimesPerPop))
X$OtherPerCap[is.na(X$OtherPerCap)] <- mean(X$OtherPerCap, na.rm = TRUE)
predictors <- names(X)

# combine feature matrix and response vector for convenience
dat <- as.data.frame(cbind(X,y))
```

## Three-way hold-out method

Before we begin implementing any of the regularization methods, we first split the data into three different parts:  

1. Training set: $60$% of the data (chosen at random)
2. Validation set: $20$% of the data (i.e. one half of the remaining $40$% not in training, chosen at random) 
3. Test set: $20$% of the data (i.e. the other half of the remaining $40$% not in training, chosen at random)  

```{r}
set.seed(1)
train_prop <- 0.6
total <- nrow(dat)

# training set
train <- sample(1:total, floor(total*train_prop))
testval <- c(1:total)[-train]

# validation set
validation <- sample(testval, size = floor(length(testval)/2))

# test set
test <- c(1:total)[-c(train, validation)]
```

## PCA 

After successfully prepping our data, we standardize it in order to ensure that the significance of each variable is properly captured relative to one another. The first thing that we would like to do is attempt to reduce the dimensionality by performing PCA.

```{r}
# standardize feature matrix
X <- scale(X)

# compute variance-covariance matrix
n <- nrow(X)
S <- (1/(n-1)) * t(as.matrix(X)) %*% as.matrix(X)

# perform eigenvalue decomposition
EVD <- eigen(S)

# eigenvalues of S (the variances of each PC)
evalues <- EVD$values

# eigenvectors of S (loadings)
evectors <- EVD$vectors

# principal components (each PC is a linear combination of each of the original features)
pcs <- X %*% evectors
colnames(pcs) <- paste0('PC', 1:ncol(pcs))
#head(pcs[,1:4])

# high level summary of doing PCA
pc <- princomp(X, scores = TRUE)
summary(pc)
```

We can nicely summarize our findings visually with a plot which will give us a sense of what PCs are important and the amount of variance each captures.

```{r, fig.align = "center", fig.pos = "H", fig.height = 3}
# plot of principal components
plot(pc, type = "l")
```

The plot only shows up to the first nine PCs. Despite that, we notice the quick drop off in variance captured which indicates that we do not need to use all of the PCs to get good results since the first couple of PCs already capture most of the variance of our original data. In fact, if we look at the cumulative proportions, we notice that the first $17$ PCs already capture roughly $85$% of the total variance and the first $34$ captures about $95$%! This is a big step forward since it shows a significant drop in the number of PCs we would need (total of $102$ PCs), reducing dimensionality while still capturing as much variability of the original data as possible. We can build our model using just a few PCs and still get good results; this ultimately sets the stage for dimension reduction techniques like PCR as well as PLSR and gives us a reason as to why we should utilize them.

# Methods 

## Principal Component Regression

In the first method, we will use Principal Component Regression (PCR). The idea is that we can fit a least squares model on a small number of PCs and obtain coefficients that give us a good model to use in predicting our response. We use the training set to fit PCR. The `pcr` function returns the regression coefficients in terms of the X variables, which simplifies interpretation. We can then visualize how the coefficients grow in terms of the number of retained PCs.

```{r, fig.align = "center", fig.pos = "H", fig.height = 4}
formula <- y ~. 
Xtrain <- scale(dat[train, c(predictors, response)])
Xtrain <- as.data.frame(Xtrain)

# PCR on training data
pcr_fit <- pcr(formula, data = Xtrain, scale = FALSE, ncomp = ncol(X))

PCR <- apply(pcr_fit$coefficients, MARGIN = 3, function(u) u)
matplot(t(PCR), type = "l", xlim = c(1, 102), ylim = c(-1, 1), ylab = "Value", xlab = "PCs")
```

The graph consists of a different line for each coefficient (total of $102$ because there are $102$ features). Each line captures how each of the coefficients change as the number of retained PCs increases. The coefficients start off stable but as the number of PCs increase, we can see that the values of the coefficients start to grow both in the postive and negative directions. We observe something interesting when the number of PCs reach $60$. We can see that at that point, the values of the coefficients begin to fluctuate really rapidly, leading to significant differences between them. 

## Partial Least Squares Regression

The next method is Partial Least Squares Regression (PLSR). It is very similar to PCR except in PLS, it makes use of the response in order to identify new features (PLS components) that not only approximate the old features well, but also that are related to the response. The function `plsr` returns the regression coefficients in terms of the X variables, which simplifies interpretation yet again. Similar to what we did with PCR, We can visualize how the coefficients grow in terms of the number of retained PLS components.

```{r, fig.align = "center", fig.pos = "H", fig.height = 4}
# PLSR on training data
plsr_fit <-plsr(formula, data = Xtrain, scale = FALSE, ncomp = ncol(X))

PLSR <- apply(plsr_fit$coefficients, MARGIN = 3,function(u) u)
matplot(t(PLSR), type = "l", xlim = c(1, 102), ylim = c(-1.5, 1.25), ylab = "Value", xlab = "PLS Components")
```

The graph consists of a different line for each coefficient (total of $102$ because there are $102$ features). Each line captures how each of the coefficients change as the number of retained PLS components increases. Unlike the coefficients in PCR, we notice that the cofficients in PLS experience much greater changes and are much more sensitive to the number of components retained.

## Ridge Regression

So far, we fitted two dimension reduction methods on our training data and observed how the regression coefficients change when we retain different numbers of components for PCR and PLSR. We can also use shrinkage methods like Ridge regression (RR) and Lasso regression to examine some interesting effects that the tuning parameter, lambda, has on the regression coefficients. This way, we will be able to have multiple sources of comparision, providing us more insight about the different regularization techniques used. Below is a graph of how the regression coefficients change in terms of lambda for Ridge regression.

```{r, fig.align = "center", fig.pos = "H", fig.height = 4}
# ridge regression on training data
x <- as.matrix(Xtrain[ ,predictors])
y <- Xtrain[ ,response]

ridge_fit <- glmnet(x, y, alpha = 0)

matplot(t(ridge_fit$beta), type = "l", las = 1, ylim = c(-0.1, 0.2), ylab = "Value", xlab = "Lambda")
```

The graph consists of a different line for each coefficient (total of $102$ because there are $102$ features). Each line captures how each of the coefficients change with varying values of lambda. We notice that the change in value of the cofficients for RR follow a similar shape as that of PLSR and PCR. 

## Lasso Regression

Lasso regression is similar to Ridge regression in the sense that both are shrinkage methods that involve the parameter, lambda. However, one disadvantage that Ridge regression has is the fact that RR will include all features (in this case $102$) in the final model. Although this may not be a problem for prediction accuracy, it may make model interpretation somewhat challenging especially when the number of features we are working with is large. Lasso regression helps overcome this issue by slightly adjusting the way the penalty is set up. Below is a graph of how the regression coefficients change in terms of lambda for Lasso regression. 

```{r, fig.align = "center", fig.pos = "H", fig.height = 4}
# lasso regression on training data
lasso_fit <- glmnet(x, y, alpha = 1)

matplot(t(lasso_fit$beta), type = "l", las = 1, ylab = "Value", xlab = "Lambda")
```

The graph consists of a different line for each coefficient (total of $102$ because there are $102$ features). Each line captures how each of the coefficients change with varying values of lambda. Again, we notice here that the change in value of the cofficients for Lasso follow a similar shape as that of PLSR and PCR. The one thing that stands out about the regression coefficients for Lasso is how sensitive it is to lambda early on.  

# Cross-Validation

The question to ask now is: what is the best number of components to have? What is the best lambda to use? The answers to these questions can be found by using $k$-fold cross validation. The size of $k$ is somewhat arbitrary, but typically $5$ or $10$ is good. We will go with $k=5$ here. That means we will have to create folds by splitting the training data into $5$ subsets of similar size and then train on the first $k-1$ folds and reserve the kth fold for "testing". We repeat this process for each fold, training on all folds except for the one that has been reserved for "testing".

```{r}
# create folds
folds <- 5
if ((length(train)%%folds)==0){
  cuts <- seq(1,length(train), by = length(train)/folds)
} else {
    cuts <- seq(1,length(train), by = floor(length(train)/folds))
    cuts[folds+1] <-length(train)
}

# starting and ending positions of folds
fold_starts <- cuts[-length(cuts)]
fold_ends <- c(cuts[2:(length(cuts)-1)]-1, cuts[length(cuts)])

folds_list <-as.list(1:folds)
for(fold in 1:folds) {
  folds_list[[fold]] <- fold_starts[fold]:fold_ends[fold]
}
```

Once we have created the folds, we will train our methods on the training data and compute cross-validation MSEs for each value of a tuning parameter (the number of components and lambda are examples of tuning parameters). We note the tuning parameters that yield the lowest cross-validation MSE for each model (training MSE for each). 

```{r}
# the following code uses a for loop to do this process for PCR and PLSR. We also make use of 
# the `cv.glmnet` function to do this process for Ridge regression and Lasso regression.
mse_cv_pcr <- c()
mse_cv_plsr <- c()

# pcr and plsr
for(tune in 1:ncol(X)) {
  mse_pcr_aux <- c()
  mse_plsr_aux <- c()
  for(fold in 1:folds) {
    # train with k-1 folds
    Xtrain_folds <- scale(Xtrain[folds_list[[fold]], ])
    Xtrain_folds <- as.data.frame(Xtrain_folds)
    x_aux <- as.matrix(Xtrain_folds[, predictors])
    y_aux <- Xtrain_folds[,response]
    pcr_aux <- pcr(formula, data = Xtrain_folds, scale = FALSE, ncomp = tune)
    plsr_aux <-plsr(formula, data = Xtrain_folds, scale = FALSE, ncomp = tune)
    
    # predict holdout fold
    Xfoldout <- scale(Xtrain[unlist(folds_list[-fold]), predictors])
    Xfoldout <- as.data.frame(Xfoldout)
    yfoldout <- Xtrain[unlist(folds_list[-fold]), response]
    
    pcr_hat <- predict(pcr_aux, Xfoldout, ncomp = tune)
    plsr_hat <- predict(plsr_aux, Xfoldout, ncomp = tune)
    
    # measure performance
    mse_pcr_aux[fold] <- mean((pcr_hat[,,1]-yfoldout)^2)
    mse_plsr_aux[fold] <- mean((plsr_hat[,,1]-yfoldout)^2)
  }
  mse_cv_pcr[tune] <- mean(mse_pcr_aux)
  mse_cv_plsr[tune] <- mean(mse_plsr_aux)
}

lasso <- cv.glmnet(as.matrix(Xtrain[,c(1:102)]), Xtrain[,response], alpha = 1, type.measure = "mse", nfolds = 5)
ridge <- cv.glmnet(as.matrix(Xtrain[,c(1:102)]), Xtrain[,response], alpha = 0, type.measure = "mse", nfolds = 5)

# lasso
lasso_keep <- lasso$nzero[lasso$lambda == lasso$lambda.min]

# ridge
ridge_keep <- ridge$nzero[ridge$lambda == ridge$lambda.min]

# show the best tuning parameter for each method
cv_mse <- data.frame(pcr = mse_cv_pcr, plsr = mse_cv_plsr)
selected_pcr_plsr <- apply(cv_mse, 2, which.min)
keep <- c(selected_pcr_plsr, "ridge" = ridge$lambda.min, "lasso" = lasso$lambda.min)
keep
```

The above table summarizes, for every method, the best number of components to keep as well as the best lambda to choose (determined by the lowest cross-validation MSE for each). Now that we have found the best tuning parameters for each model, the next question to ask is: which model is the best one? We will use the validation set to determine the answer. In the next step, we will fit our ideal models (those with the best tuning parameters that we just found for each) on our validation set and compare how each model does.  

```{r}
Xval <- scale(dat[validation, c(predictors, response)])
x_val <- as.matrix(Xval[,predictors])
y_val <- Xval[, response]

pcr_hat <- predict(pcr_fit, x_val, ncomp = keep[1])
plsr_hat <- predict(plsr_fit, x_val, ncomp = keep[2])
las_hat <- predict(lasso_fit, newx = x_val, s = keep[3])
rr_hat <- predict(ridge_fit, newx = x_val, s = keep[4])

mse_val <- c(
  "pcr_mse" = mean((pcr_hat[,,1]-y_val)^2),
  "plsr_mse" = mean((plsr_hat[,,1]-y_val)^2),
  "ridge_mse" = mean((rr_hat[,1]-y_val)^2),
  "lasso_mse" = mean((las_hat[,1]-y_val)^2)
)

mse_val
#which.min(mse_val)
```

The model that gives us the lowest validation MSE is the best model out of the bunch; in our case, Ridge Regression yields the lowest validation MSE. Once we have identified the best model, we need to assess its performance using the test set. 

# Final Model Performance

```{r}
# test the performance of the best model
Xbest <- scale(dat[c(train, validation),c(predictors, response)])
Xtest <- scale(dat[test,c(predictors, response)])
x_test <- as.matrix(Xtest[, predictors])
y_test <- Xtest[,response]

ridge_best <- glmnet(Xbest[,predictors], Xbest[,response], alpha = 0, lambda = keep[4])
y_hat_test <- predict(ridge_fit, newx = x_test, s = keep[4])
#mean((y_hat_test-y_test)^2)
```

The test MSE for our best model (Ridge regression) is around 0.33. We note that the test MSE is greater than the validation MSE, but by a very small amount. We can conclude that our model did a fairly good job in predicting "ViolentCrimesPerPop". For our last step, we will use our entire dataset (training set, validation set, and test set) to fit our crowned model and observe its coefficients.  

```{r}
Xall <- scale(dat)
ridge_model <- glmnet(Xall[,predictors], Xall[,response], alpha = 0, lambda = keep[4])
ridge_model$beta
```
